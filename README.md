Download Link: https://assignmentchef.com/product/solved-cs6476-project4-depth-estimation-using-stereo
<br>
The goal of this project is to create stereo depth estimation algorithms, both classical and deep learning based. For classical stereo depth estimation algorithms, you will be using deterministic functions to compare patches and compute a disparity map. For deep learning based algorithms, you will be using a learning method to estimate the disparity map. There will be two parts in this project, the first of which is described in this handout. You will implement functions in part1_<sub>*</sub>.py to generate random patches, evaluate the similarity of those patches, and then compute the disparity map for several images. The corresponding notebook for this section is part1_simple_stereo.ipynb. Part 2 of this project (including its corresponding handout) will be released separately.

<h1>Setup</h1>

<ol>

 <li>Install <a href="https://conda.io/miniconda.html">Miniconda</a><a href="https://conda.io/miniconda.html">.</a> It doesn’t matter whether you use Python 2 or 3 because we will create our own environment that uses python3 anyways.</li>

 <li>Download and extract the part 1 starter code.</li>

 <li>Create a conda environment using the appropriate command. On Windows, open the installed “Conda prompt” to run the command. On MacOS and Linux, you can just use a terminal window to run the command, Modify the command based on your OS (linux, mac, or win): conda env create -f</li>

</ol>

proj4_env_&lt;OS&gt;.yml. If you’re running into issues building the environment, try running conda update –all first.

<ol start="4">

 <li>This will create an environment named “cs6476 proj4”. Activate it using the Windows command, activate cs6476_proj4 or the MacOS / Linux command, conda activate cs6476_proj4 or source activate cs6476_proj4</li>

 <li>Install the project package, by running pip install -e . inside the repo folder. This might be unnecessary for every project, but is good practice when setting up a new conda environment that may have pip</li>

 <li>Run the notebook using jupyter notebook ./proj4_code/part1_simple_stereo.ipynb</li>

 <li>After implementing all functions, ensure that all sanity checks are passing by running pytest proj4_unit_tests inside the repo folder.</li>

 <li>Complete part 2 (template and instructions to be released separately).</li>

</ol>

<h1>1           Simple stereo by matching patches</h1>

<h2>Introduction</h2>

We know that there is some encoding of depth when images are captured using a stereo rig, much like human eyes. You can try a simple experiment to see the stereo effect in action. Try seeing a scene with only your left eye. Then close your left eye and see using your right eye. Make the transition quickly. You should notice a <em>horizontal </em>shift in the image perceived. Can you comment on the difference in shift for different objects when you do this experiment? Is it related to the depth of the objects in some way?

In this section, we will generate a <strong>disparity map</strong>, which is the map of horizontal shifts estimated at each pixel. We will start working on a simple algorithm, which will then be improved to calculate more accurate disparity maps.

The notebook corresponding to this part is part1_simple_stereo.ipynb.

<h2>1.1         Random dot stereogram</h2>

It was once believed that in order to perceive depth, one must either match feature points (like SIFT) between left and right images, or rely upon clues such as shadows.

A random dot stereogram eliminates all other depth cues, and hence proves that a stereo setup is sufficient to get an idea of the depth of the scene. A random dot stereogram is generated by the following steps:

<ol>

 <li>Create the left image with random dots at each pixel (0/1 values).</li>

 <li>Create the right image as a copy of the left image.</li>

 <li>Select a region in the right image and shift it horizontally.</li>

 <li>Add a random pattern in the right image in the empty region created after the shift.</li>

</ol>

In part1a_random_stereogram.py, you will implement generate_random_stereogram() to generate a random dot stereogram for the given image size.

<h2>1.2         Similarity measure</h2>

To compare patches between left and right images, we will need two kinds of similarity functions:

<ol>

 <li>Sum of squared differences (SSD):</li>

</ol>

<em>SSD</em>(<em>A,B</em>) = <sup>X </sup>(<em>A<sub>ij </sub></em>− <em>B<sub>ij</sub></em>)<sup>2                                                                                                             </sup>(1)

<em>i</em>∈[0<em>,H</em>)<em>,j</em>∈[0<em>,W</em>)

<ol start="2">

 <li>Sum of absolute differences (SAD):</li>

</ol>

<em>SAD</em>(<em>A,B</em>) = <sup>X </sup>|<em>A<sub>ij </sub></em>− <em>B<sub>ij</sub></em>|                                                                         (2)

<em>i</em>∈[0<em>,H</em>)<em>,j</em>∈[0<em>,W</em>)

where <em>A </em>and <em>B </em>are two patches of height <em>H </em>and width <em>W</em>.

In part1b_similarity_measures.py, you will implement the following:

<ul>

 <li>ssd_similarity_measure(): Calculate SSD distance.</li>

 <li>sad_similarity_measure(): Calculate SAD distance.</li>

</ul>

<h2>1.3         Disparity maps</h2>

We are now ready to write code for a simple algorithm for stereo matching. You will need to follow the steps visualized in Figure 1:

Figure 1: Example of a stereo algorithm.

<ol>

 <li>Pick a patch in the left image (red block), <em>P</em><sub>1</sub>.</li>

 <li>Place the patch in the same (<em>x,y</em>) coordinates in the right image (red block). As this is binocular stereo, we will need to search for <em>P</em><sub>1 </sub>on the left side starting from this position. Make sure you understand this point well before proceeding further.</li>

 <li>Slide the block of candidates to the left (indicated by the different pink blocks). The search area is restricted by the parameter max_search_bound in the code. The candidates will overlap.</li>

 <li>We will pick the candidate patch with the minimum similarity error (green block). The horizontal shift from the red block to the green block in this image is the disparity value for the center of <em>P</em><sub>1 </sub>in the left image.</li>

</ol>

Note: the images have already been rectified, so we can search only a single horizontal scan line.

In part1c_disparity_map.py, you will implement calculate_disparity_map() (please read the documentation carefully!) to calculate the disparity value at each pixel by searching a small patch around a pixel from the left image in the right image.

(a) Convex profile.                                                          (b) Non-convex profile.

Figure 2

<h2>1.4         Error profile analysis</h2>

Before computing the full disparity map, we will first analyze the similarity error distribution between patches. You will have to find two examples which display a close-to-convex error profile, and a highly non-convex profile, respectively. For reference, we provide the plots we obtained (see Figure 2). Based on your output visualizations and understanding of the process, answer the reflection questions in the report.

<h2>1.5         Real life stereo images</h2>

You will iterate through pairs of images from the dataset and calculate the disparity maps for images. The code is already given to you. You just need to compare the disparity maps and answer the reflection questions in the report.

<h2>1.6         Smoothing</h2>

One issue with the above results is that they aren’t very smooth. Pixels next to each other on the same surface can have vastly different disparities, making the results look very noisy and patchy in some areas. Intuitively, pixels next to each other should have a smooth transition in disparity (unless at an object boundary or occlusion).

In this part, we try to improve our results. One way of doing this is through the use of a smoothing constraint. The smoothing method we use is called semi-global matching (SGM) or semi-global block matching. Before, we picked the disparity for a pixel based on the minimum matching cost of the block using some metric (SSD or SAD). Now, the basic idea of SGM is to penalize disparity computations which are very different than their pixel-wise neighbors by adding a penalty term on top of the matching cost term. SGM approximately minimizes the global (over the entire image) energy function.

<em>E</em>(<em>D</em>) ≤ <sup>X</sup>(<em>C</em>(<em>p,D<sub>p</sub></em>) + <sup>X</sup><em>PT</em>(|<em>D<sub>p </sub></em>− <em>D<sub>q</sub></em>|))

<em>p                                           q</em>

<em>C</em>(<em>p,D<sub>p</sub></em>) is the matching cost for a pixel with disparity <em>D<sub>p</sub></em>, <em>q </em>is a neighboring pixel, and <em>PT</em>(·) is some penalty function penalizing the difference in disparities. You can read more about how this method works and is optimized here: <a href="https://elib.dlr.de/73119/1/180Hirschmueller.pdf">Semi-Global Matching – Motivation, Developments, and Applications</a> and <a href="https://pdfs.semanticscholar.org/bcd8/4d8bd864ff903e3fe5b91bed3f2eedacc324.pdf">Stereo </a><a href="https://pdfs.semanticscholar.org/bcd8/4d8bd864ff903e3fe5b91bed3f2eedacc324.pdf">Processing by Semi-Global Matching and Mutual Information</a><a href="https://pdfs.semanticscholar.org/bcd8/4d8bd864ff903e3fe5b91bed3f2eedacc324.pdf">.</a>

You will not need to implement SGM by yourself. But to help understand SGM, you will implement a function which computes the <strong>cost volume</strong>. You have already written code to compute disparity map. Now you will extend that code to compute the cost volume. Instead of taking the argmin of the similarity error profile, we will store the tensor of error profiles at each pixel location along the third dimension. If we have an input image of dimension (H,W,C) and max search bound of <em>D</em>, the cost_volume will be a tensor of dimension (H,W,D). The cost volume at (<em>i,j</em>) pixel is the error profile obtained for the patch in the left image centered at (<em>i,j</em>).

In part1c_disparity_map.py, you will implement calculate_cost_volume() to calculate the disparity map.

<h1>Testing</h1>

We have provided a set of tests for you to evaluate your implementation. We have included tests for part

1 inside part1_simple_stereo.ipynb so you can check your progress as you implement each function. When you’re done with the entire project, you can call additional tests by running pytest proj4_unit_tests inside the root directory of the project, as well as checking against the tests on Gradescope. <em>Your grade on the coding portion of the project will be further evaluated with a set of tests not provided to you.</em>

<ul>

 <li>Project 4 part 2The goal of this project is to create stereo depth estimation algorithms, both classical and deep learning based. In part 1, you implemented classical stereo depth estimation algorithms using a deterministic function to evaluate patches and then get disparity map. In part 2, you will implement deep learning based algorithms to estimate the disparity map. Specifically, you will 1) implement the part for generating patch and architecture of MC-CNN model in part2_<sub>*</sub>.py and go through part2_disparity.ipynb. Make sure you pass all the sanity checks for part 2 before starting training. 2) use part2_mc_cnn.ipynb to go through the training and visualize the results of your model.<h1>Setup</h1><strong>You can skip steps 1-5 if you’ve already started part 1.</strong>

  <ol>

   <li>Install <a href="https://conda.io/miniconda.html">Miniconda</a><a href="https://conda.io/miniconda.html">.</a> It doesn’t matter whether you use Python 2 or 3 because we will create our own environment that uses python3 anyways.</li>

   <li>Download and extract the project starter code.</li>

   <li>Create a conda environment using the appropriate command. On Windows, open the installed “Conda prompt” to run the command. On MacOS and Linux, you can just use a terminal window to run the command, Modify the command based on your OS (linux, mac, or win): conda env create -f</li>

  </ol>proj4_env_&lt;OS&gt;.yml

  <ol start="4">

   <li>This will create an environment named “cs6476 proj4”. Activate it using the Windows command, activate cs6476_proj4 or the MacOS / Linux command, conda activate cs6476_proj4 or source activate cs6476_proj4</li>

   <li>Install the project package, by running pip install -e . inside the repo folder. This might be unnecessary for every project, but is good practice when setting up a new conda environment that may have pip</li>

   <li>Section 2:

    <ul>

     <li>Part 1: Run the notebook using jupyter notebook ./proj4_code/part2_disparity.ipynb</li>

     <li>Part 2: Run the notebook part2_mc_cnn.ipynb and upload the zipped (semiglobalmatching and proj4_code) as proj4.zip to Colab.</li>

    </ul></li>

   <li>Once you are done executing the Colab notebook, and are satisfied with your visualization, run the final cell, which will generate a file called pth. Download it and make sure you save that file in your proj4_code folder while submitting, as we will evaluate your model’s performance as a hidden Gradescope test.</li>

   <li>Generate the zip folder for the code portion of your submission once you’ve finished the project using python zip_submission.py –gt_username &lt;your_gt_username&gt;</li>

  </ol><h1>2       Learning-based stereo matching</h1>In the previous section, you saw how we can use simple concepts like <em>SAD </em>and <em>SSD </em>to compute matching costs between two patches and produce disparity maps. Now let’s try something different – instead of using <em>SAD </em>or <em>SSD </em>to measure similarity, we will train a neural network and learn from the data directly.<h2>Introduction</h2>You’ll implement what has been proposed in the paper <a href="https://arxiv.org/abs/1409.4326">[Zbontar &amp; LeCun,</a> <a href="https://arxiv.org/abs/1409.4326">2015]</a><a href="https://arxiv.org/abs/1409.4326">,</a> and evaluate how it performs compared to classical cost matching approaches. The paper proposes several network architectures, but what we will be using is the accurate architecture for the <a href="http://vision.middlebury.edu/stereo/data/">Middlebury stereo dataset</a><a href="http://vision.middlebury.edu/stereo/data/">.</a> This dataset provides a ground truth disparity map for each stereo pair, which means we know exactly where the match is supposed to be on the epipolar line. This allows us to extract many such matches and train the network to identify what type of patches should be matches and what shouldn’t. You should definitely read the paper in more details if you’re curious about how it works.You don’t have to worry about the dataset – we provide images in a ready-to-use format (with rectification). In fact, you won’t be doing much coding in this part. Rather, you should focus on experimenting and thinking about <em>why</em>. Your report will have a lot of weight in this part, so try to be as clear as possible.Note: The network in Part 2.2.1 can take around 15-30 mins to train on Colab. We suggest you <strong>start early and don’t wait until the last minute</strong>.<h2>2.1        PyTorch functions on CPU</h2>In this part, we will implement an MCNET network architecture as described in the paper (See Figure 1), generate patches for the training process, and calculate disparity for MCNET.The corresponding notebook for this part is part2_disparity.ipynb.<strong>2.1.1        Network architecture</strong><h3>MCNET</h3>We will follow the description of the “accurate” network for Middlebury dataset. The inputs to the network are 2 image patches, coming from left and right images. Each will pass through a series of convolution + ReLU layers. The extracted features are then concatenated and passed through additional fully connected + ReLU layers. The output is a single real number between 0 and 1, indicating the similarity between the two input images <a href="https://arxiv.org/abs/1409.4326">[Zbontar &amp; LeCun,</a> <a href="https://arxiv.org/abs/1409.4326">2015]</a><a href="https://arxiv.org/abs/1409.4326">.</a> In this case, since training from scratch will take a really long time to converge, you’ll train from our pre-trained network instead. In order to load up the pre-trained network, you must first implement the architecture exactly as described below:Figure 1: Visualization of network architecture.For efficiency we will convolve both input images in the same batch (this means that the input to the network will be 2×<em>batch size</em>). After the convolutional layers, we will then reshape them into [<em>batch size,conv out</em>] where <em>conv out </em>is the flattened output size of the convolutional layers. This will then be passed through a series of fully connected layers and finally a sigmoid layer to bound the output value to [0<em>,</em>1].Here is an example of a network with <em>num conv layers </em>= 1 and <em>num fc layers </em>= 2:conv_layers = nn.Sequential( nn.Conv2d(in_channel, num_feature_map, kernel_size=kernel_size, stride=1, padding=( kernel_size // 2)), nn.ReLU(), )fully_connected_layers = nn.Sequential( nn.Linear(conv_out, num_hidden_units), nn.ReLU(), nn.Linear(num_hidden_units, 1), nn.Sigmoid())conv_feature_batch = conv_layers(input_batch) conv_feature_batch.reshape((batch_size, conv_out) output_batch = fully_connected_layers(conv_feature_batch)In part2a_network.py, you will implement the following network architecture:

  <ul>

   <li>MCNET: Implement the network architecture as described in the paper.</li>

  </ul><strong>2.1.2        Patch generation</strong>In part2b_patch.py, you will implement gen_patch() to extract a patch from an image.<h3>2.1.3            Disparity map calculation with MCNET</h3>The core logic for calculating disparity for MCNET will remain the same, but we will have to do a few things differently. It will take around 1-2 mins to generate the disparity map if implemented correctly. The steps required here are as follows:

  <ol>

   <li>We will operate on convolutional features instead of raw pixels. Pass the images through the convolutional block of MCNET to obtain the features.</li>

   <li>Pick a patch in the left image features, <em>P</em><sub>1</sub>.</li>

   <li>Calculate the search-space of corresponding patches in the right image features:

    <ul>

     <li>As before, place the patch in the corresponding location in the right image features, and slide it to obtain a sequence of window patches.</li>

     <li>Concatenate these patches at the 0th dimension to form a batch of patches.</li>

    </ul></li>

   <li>Compute the similarity values over the entire window using the similarity function provided to you. All the similarity values over the window will be present as a (<em>k </em>× 1) tensor.</li>

   <li>Pick the patch with the minimum similarity error.</li>

  </ol>Note: It is important that the similarity calculation happens in parallel over the entire search window. Otherwise, the disparity calculation will take a really long time in the subsequent part.In part2c_disparity.py, you will implement mc_cnn_similarity() and calculate_mccnn_cost_volume() to calculate the disparity value at each pixel using MCNET.Note: Before proceeding to the next part, you need to ensure that <strong>all sanity checks for this part are passing </strong>by running part2_disparity.ipynb with jupyter notebook, and running pytest proj4_unit_tests<h2>2.2        Train and evaluation on Google Colab</h2>In this part, we will train the MCNET architecture and evaluate the overall performance.<h3>Setup</h3>We will be using <a href="https://colab.research.google.com/">Google Colab</a><a href="https://colab.research.google.com/">,</a> which is a cloud-based Jupyter notebook environment. You can choose to run this section locally as well, especially if you have a good GPU, but the assignment is designed to run on Colab with GPU (this project is doable without a GPU, but a GPU makes the process much faster and frustration free.). These are the steps we follow:

  <ol>

   <li>Upload ipynb to Google Colab</li>

   <li>Zip semiglobalmatching and proj4_code into zip and upload them to the Colab runtime.</li>

   <li>Unzip the uploaded zip using !unzip -qq uploaded_file.zip -d ./</li>

   <li>In Colab, make sure you select “GPU” in the menu (“Runtime” → “Change runtime time” → “Hardware accelerator”).</li>

  </ol>You will need to follow the instructions in Setup, Compute Requirements, and DataLoader in the notebook to download the necessary data and set up the environment in Google Colab.<h3>2.2.1         Train MCNET</h3>In this part, we will train a neural network that learns how to classify 2 patches as positive vs negative match. Your task is to train a best network by experimenting with the learning parameters. The following shows the experiments you need to complete for this part:

  <ul>

   <li>Experiment with the learning rate: try using large (<em>&gt; </em>1) vs. small (<em>&lt; </em>1<em>e </em>− 5) values. Based on your output visualizations, answer the reflection questions in the report.</li>

   <li>Experiment with the window size: In the previous part, we use window size of 11 as suggested in the paper, meaning that the input to the network will be patches of size 11×11. This corresponds to the block size that will be used when perform stereo matching later on. You can experiment with other window size, namely 5×5, 9×9, and 15×15 and compare the performance.</li>

   <li>Tune the training parameters and pick the best combination of hyperparameters with the best disparity map visualization. You should show the training loss plot in the report and answer the reflection questions in the report. <strong>Typically, models with average error of around 20 tend to pass the Gradescope tests</strong>.</li>

  </ul><h3>2.2.2        Evaluate stereo matching</h3>In this part, we will again generate the disparity map but this time from our newly trained matching cost network. We will use calculate_mc_cnn_disparity from part2c_disparity.py for this.Note that all the required functions in Part 2.1 need to be implemented correctly before starting this part.Hint: You don’t have to re-train the network every time you want to evaluate, as long as your saved model is in Colab file system. Don’t forget to change load_path to your best model.Then we will evaluate your trained network as a stereo matching cost with the metrics used in the <a href="http://vision.middlebury.edu/stereo/eval3/">Middle</a><a href="http://vision.middlebury.edu/stereo/eval3/">bury leaderboard for stereo matching</a><a href="http://vision.middlebury.edu/stereo/eval3/">.</a> For the bicycle image, you should see the improvement in using the trained network vs. <em>SAD </em>cost matching.

  <ul>

   <li><em>avgerr</em>: average absolute error in pixels (lower is better)</li>

   <li><em>bad1</em>: percentage of bad pixels whose error is <em>&gt; </em>1 (lower is better)</li>

   <li><em>bad2</em>: percentage of bad pixels whose error is <em>&gt; </em>2 (lower is better)</li>

   <li><em>bad4</em>: percentage of bad pixels whose error is <em>&gt; </em>4 (lower is better)</li>

  </ul><h3>Evaluate stereo matching with SGM</h3>You will use the semi-global matching module in part1 and the calculate_mccnn_cost_volume in part2c_disparity .py to evaluate the disparity map generated by SAD method and MC-CNN model.Based on your outputs, answer the reflection questions in the report.<h1>3      Writeup</h1>For this project (and all other projects), you must do a project report using the template slides provided to you. Do <em>not </em>change the order of the slides or remove any slides, as this will affect the grading process on Gradescope and you will be deducted points. In the report you will describe your algorithm and any decisions you made to write your algorithm a particular way. Then you will show and discuss the results of your algorithm. The template slides provide guidance for what you should include in your report. A good writeup doesn’t just show results–it tries to draw some conclusions from the experiments. You must convert the slide deck into a PDF for your submission.<h1>Testing</h1>We have provided a set of tests for you to evaluate your implementation. We have included tests inside part1_simple_stereo.ipynb and part2_disparity.ipynb so you can check your progress as you implement each section. <em>Your grade on the coding portion of the project will be further evaluated with a set of tests not provided to you.</em><h1>Bells &amp; whistles (extra points)</h1>Also note that while we’re closely following the proposed matching cost network from the paper, we’re still skipping several bells and whistles post-processing components used in the paper, so the results are still far from perfect. You are free to add any components you think would be useful (doesn’t have to be from the paper). There is a maximum of 10 pts extra credit for any interesting experiments beyond the outline we have. Be creative, and be clear and concise about what extra things you did in the report. Here are some starting points:

  <ul>

   <li>Data augmentation for training: Rather than cropping patches directly, you can augment the data with slight rotation/affine transformations to make it more robust to noise and perspective change. Be careful not to transform too much that we lose the precision of the match. To earn extra credit, you must explain the augmentation you do, and show the improvement with adding it.</li>

   <li>Experiment with training from scratch: Get great performance by training from scratch using datasets like KITTI2015.</li>

  </ul>If you choose to do anything extra, <strong>include your code implementation in </strong><strong>proj4_code/extra_credit.py</strong>, and add slides <em>after the slides given in the template deck </em>to describe your implementation, results, and analysis. Adding slides in between the report template will cause issues with Gradescope, and you will be deducted points. You will not receive full credit for your extra credit implementations if they are not described adequately in your writeup. </li>

</ul>